{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import model\n",
    "import data\n",
    "import torch\n",
    "import time\n",
    "from pathlib import Path\n",
    "import tqdm\n",
    "import json\n",
    "import utils\n",
    "import sklearn.preprocessing\n",
    "import numpy as np\n",
    "import random\n",
    "from git import Repo\n",
    "import os\n",
    "import copy\n",
    "\n",
    "\n",
    "tqdm.monitor_interval = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, unmix, device, train_sampler, optimizer):\n",
    "    losses = utils.AverageMeter()\n",
    "    unmix.train()\n",
    "    pbar = tqdm.tqdm(train_sampler, disable=args.quiet) \n",
    "    for x, y in pbar:\n",
    "        pbar.set_description(\"Training batch\")\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        Y_hat = unmix(x)\n",
    "        Y = unmix.transform(y)\n",
    "        loss = torch.nn.functional.mse_loss(Y_hat, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.update(loss.item(), Y.size(1))\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(args, unmix, device, valid_sampler):\n",
    "    losses = utils.AverageMeter()\n",
    "    unmix.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in valid_sampler:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            Y_hat = unmix(x)\n",
    "            Y = unmix.transform(y)\n",
    "            loss = torch.nn.functional.mse_loss(Y_hat, Y)\n",
    "            losses.update(loss.item(), Y.size(1))\n",
    "        return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statistics(args, dataset):\n",
    "    scaler = sklearn.preprocessing.StandardScaler()\n",
    "\n",
    "    spec = torch.nn.Sequential(\n",
    "        model.STFT(n_fft=args.nfft, n_hop=args.nhop),\n",
    "        model.Spectrogram(mono=True)\n",
    "    )\n",
    "\n",
    "    dataset_scaler = copy.deepcopy(dataset)\n",
    "    dataset_scaler.samples_per_track = 1\n",
    "    dataset_scaler.augmentations = None\n",
    "    dataset_scaler.random_chunks = False\n",
    "    dataset_scaler.seq_duration = None\n",
    "    pbar = tqdm.tqdm(range(len(dataset_scaler)), disable=args.quiet)\n",
    "    for ind in pbar:\n",
    "        x, y = dataset_scaler[ind]\n",
    "        pbar.set_description(\"Compute dataset statistics\")\n",
    "        X = spec(x[None, ...])\n",
    "        scaler.partial_fit(np.squeeze(X))\n",
    "\n",
    "    # set inital input scaler values\n",
    "    std = np.maximum(\n",
    "        scaler.scale_,\n",
    "        1e-4*np.max(scaler.scale_)\n",
    "    )\n",
    "    return scaler.mean_, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreTrueAction(option_strings=['--no-cuda'], dest='no_cuda', nargs=0, const=True, default=False, type=None, choices=None, help='disables CUDA training', metavar=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Open Unmix Trainer')\n",
    "\n",
    "# which target do we want to train?\n",
    "parser.add_argument('--target', type=str, default='vocals',\n",
    "                    help='target source (will be passed to the dataset)')\n",
    "\n",
    "# Dataset paramaters\n",
    "parser.add_argument('--dataset', type=str, default=\"musdb\",\n",
    "                    choices=[\n",
    "                        'musdb', 'aligned', 'sourcefolder',\n",
    "                        'trackfolder_var', 'trackfolder_fix'\n",
    "                    ],\n",
    "                    help='Name of the dataset.')\n",
    "parser.add_argument('--root', type=str, help='root path of dataset' , default='../rec_data/')\n",
    "parser.add_argument('--output', type=str, default=\"../out_unmix/\",\n",
    "                    help='provide output path base folder name')\n",
    "parser.add_argument('--model', type=str, help='Path to checkpoint folder', default='../checkpoint_unmix')\n",
    "\n",
    "# Trainig Parameters\n",
    "parser.add_argument('--epochs', type=int, default=1000)\n",
    "parser.add_argument('--batch-size', type=int, default=16)\n",
    "parser.add_argument('--lr', type=float, default=0.001,\n",
    "                    help='learning rate, defaults to 1e-3')\n",
    "parser.add_argument('--patience', type=int, default=140,\n",
    "                    help='maximum number of epochs to train (default: 140)')\n",
    "parser.add_argument('--lr-decay-patience', type=int, default=80,\n",
    "                    help='lr decay patience for plateau scheduler')\n",
    "parser.add_argument('--lr-decay-gamma', type=float, default=0.3,\n",
    "                    help='gamma of learning rate scheduler decay')\n",
    "parser.add_argument('--weight-decay', type=float, default=0.00001,\n",
    "                    help='weight decay')\n",
    "parser.add_argument('--seed', type=int, default=42, metavar='S',\n",
    "                    help='random seed (default: 42)')\n",
    "\n",
    "\n",
    "# Model Parameters\n",
    "parser.add_argument('--seq-dur', type=float, default=6.0,\n",
    "                    help='Sequence duration in seconds'\n",
    "                    'value of <=0.0 will use full/variable length')\n",
    "parser.add_argument('--unidirectional', action='store_true', default=False,\n",
    "                    help='Use unidirectional LSTM instead of bidirectional')\n",
    "parser.add_argument('--nfft', type=int, default=4096,\n",
    "                    help='STFT fft size and window size')\n",
    "parser.add_argument('--nhop', type=int, default=1024,\n",
    "                    help='STFT hop size')\n",
    "parser.add_argument('--hidden-size', type=int, default=512,\n",
    "                    help='hidden size parameter of dense bottleneck layers')\n",
    "parser.add_argument('--bandwidth', type=int, default=16000,\n",
    "                    help='maximum model bandwidth in herz')\n",
    "parser.add_argument('--nb-channels', type=int, default=2,\n",
    "                    help='set number of channels for model (1, 2)')\n",
    "parser.add_argument('--nb-workers', type=int, default=0,\n",
    "                    help='Number of workers for dataloader.')\n",
    "\n",
    "# Misc Parameters\n",
    "parser.add_argument('--quiet', action='store_true', default=False,\n",
    "                    help='less verbose during training')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='disables CUDA training')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: True\n",
      "Using Torchaudio:  False\n"
     ]
    }
   ],
   "source": [
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "print(\"Using GPU:\", use_cuda)\n",
    "print(\"Using Torchaudio: \", utils._torchaudio_available())\n",
    "dataloader_kwargs = {'num_workers': args.nb_workers, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "repo_dir = os.path.abspath(os.path.dirname('/media/Sharedata/aniruddha/source_sep_Ani/train.py'))\n",
    "repo = Repo(repo_dir)\n",
    "commit = repo.head.commit.hexsha[:7]\n",
    "\n",
    "# use jpg or npy\n",
    "torch.manual_seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--target TARGET]\n",
      "                             [--dataset {musdb,aligned,sourcefolder,trackfolder_var,trackfolder_fix}]\n",
      "                             [--root ROOT] [--output OUTPUT] [--model MODEL]\n",
      "                             [--epochs EPOCHS] [--batch-size BATCH_SIZE]\n",
      "                             [--lr LR] [--patience PATIENCE]\n",
      "                             [--lr-decay-patience LR_DECAY_PATIENCE]\n",
      "                             [--lr-decay-gamma LR_DECAY_GAMMA]\n",
      "                             [--weight-decay WEIGHT_DECAY] [--seed S]\n",
      "                             [--seq-dur SEQ_DUR] [--unidirectional]\n",
      "                             [--nfft NFFT] [--nhop NHOP]\n",
      "                             [--hidden-size HIDDEN_SIZE]\n",
      "                             [--bandwidth BANDWIDTH]\n",
      "                             [--nb-channels NB_CHANNELS]\n",
      "                             [--nb-workers NB_WORKERS] [--quiet] [--no-cuda]\n",
      "                             [--is_wav]\n",
      "                             [--samples-per-track SAMPLES_PER_TRACK]\n",
      "                             [--source-augmentations SOURCE_AUGMENTATIONS [SOURCE_AUGMENTATIONS ...]]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /run/user/1010/jupyter/kernel-6f92f0d2-2b32-47ff-b0d1-fcb0ec0d17f7.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:3304: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "train_dataset, valid_dataset, args = data.load_datasets(parser, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-6b99f4ea0a38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m train_sampler = torch.utils.data.DataLoader(\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;34m**\u001b[0m\u001b[0mdataloader_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "target_path = Path(args.output)\n",
    "target_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_sampler = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=args.batch_size, shuffle=True,\n",
    "    **dataloader_kwargs\n",
    ")\n",
    "valid_sampler = torch.utils.data.DataLoader(\n",
    "    valid_dataset, batch_size=1,\n",
    "    **dataloader_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-7ff06faf56a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.add_argument('--iswav', action='store_true', default=True, help='loads wav instead of STEMS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Sequential"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
