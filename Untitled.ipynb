{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import model\n",
    "import data\n",
    "import torch\n",
    "import time\n",
    "from pathlib import Path\n",
    "import tqdm\n",
    "import json\n",
    "import utils\n",
    "import sklearn.preprocessing\n",
    "import numpy as np\n",
    "import random\n",
    "from git import Repo\n",
    "import os\n",
    "import copy\n",
    "\n",
    "\n",
    "tqdm.monitor_interval = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, unmix, device, train_sampler, optimizer):\n",
    "    losses = utils.AverageMeter()\n",
    "    unmix.train()\n",
    "    pbar = tqdm.tqdm(train_sampler, disable=args.quiet) \n",
    "    for x, y in pbar:\n",
    "        pbar.set_description(\"Training batch\")\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        Y_hat = unmix(x)\n",
    "        Y = unmix.transform(y)\n",
    "        loss = torch.nn.functional.mse_loss(Y_hat, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.update(loss.item(), Y.size(1))\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(args, unmix, device, valid_sampler):\n",
    "    losses = utils.AverageMeter()\n",
    "    unmix.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in valid_sampler:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            Y_hat = unmix(x)\n",
    "            Y = unmix.transform(y)\n",
    "            loss = torch.nn.functional.mse_loss(Y_hat, Y)\n",
    "            losses.update(loss.item(), Y.size(1))\n",
    "        return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statistics(args, dataset):\n",
    "    scaler = sklearn.preprocessing.StandardScaler()\n",
    "\n",
    "    spec = torch.nn.Sequential(\n",
    "        model.STFT(n_fft=args.nfft, n_hop=args.nhop),\n",
    "        model.Spectrogram(mono=True)\n",
    "    )\n",
    "\n",
    "    dataset_scaler = copy.deepcopy(dataset)\n",
    "    dataset_scaler.samples_per_track = 1\n",
    "    dataset_scaler.augmentations = None\n",
    "    dataset_scaler.random_chunks = False\n",
    "    dataset_scaler.seq_duration = None\n",
    "    pbar = tqdm.tqdm(range(len(dataset_scaler)), disable=args.quiet)\n",
    "    for ind in pbar:\n",
    "        x, y = dataset_scaler[ind]\n",
    "        pbar.set_description(\"Compute dataset statistics\")\n",
    "        X = spec(x[None, ...])\n",
    "        scaler.partial_fit(np.squeeze(X))\n",
    "\n",
    "    # set inital input scaler values\n",
    "    std = np.maximum(\n",
    "        scaler.scale_,\n",
    "        1e-4*np.max(scaler.scale_)\n",
    "    )\n",
    "    return scaler.mean_, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreTrueAction(option_strings=['--no-cuda'], dest='no_cuda', nargs=0, const=True, default=False, type=None, choices=None, help='disables CUDA training', metavar=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Open Unmix Trainer')\n",
    "\n",
    "# which target do we want to train?\n",
    "parser.add_argument('--target', type=str, default='vocals',\n",
    "                    help='target source (will be passed to the dataset)')\n",
    "\n",
    "# Dataset paramaters\n",
    "parser.add_argument('--dataset', type=str, default=\"musdb\",\n",
    "                    choices=[\n",
    "                        'musdb', 'aligned', 'sourcefolder',\n",
    "                        'trackfolder_var', 'trackfolder_fix'\n",
    "                    ],\n",
    "                    help='Name of the dataset.')\n",
    "parser.add_argument('--root', type=str, help='root path of dataset' , default='../rec_data/')\n",
    "parser.add_argument('--output', type=str, default=\"../out_unmix/\",\n",
    "                    help='provide output path base folder name')\n",
    "parser.add_argument('--model', type=str, help='Path to checkpoint folder', default='../checkpoint_unmix')\n",
    "\n",
    "# Trainig Parameters\n",
    "parser.add_argument('--epochs', type=int, default=1000)\n",
    "parser.add_argument('--batch-size', type=int, default=16)\n",
    "parser.add_argument('--lr', type=float, default=0.001,\n",
    "                    help='learning rate, defaults to 1e-3')\n",
    "parser.add_argument('--patience', type=int, default=140,\n",
    "                    help='maximum number of epochs to train (default: 140)')\n",
    "parser.add_argument('--lr-decay-patience', type=int, default=80,\n",
    "                    help='lr decay patience for plateau scheduler')\n",
    "parser.add_argument('--lr-decay-gamma', type=float, default=0.3,\n",
    "                    help='gamma of learning rate scheduler decay')\n",
    "parser.add_argument('--weight-decay', type=float, default=0.00001,\n",
    "                    help='weight decay')\n",
    "parser.add_argument('--seed', type=int, default=42, metavar='S',\n",
    "                    help='random seed (default: 42)')\n",
    "\n",
    "\n",
    "# Model Parameters\n",
    "parser.add_argument('--seq-dur', type=float, default=6.0,\n",
    "                    help='Sequence duration in seconds'\n",
    "                    'value of <=0.0 will use full/variable length')\n",
    "parser.add_argument('--unidirectional', action='store_true', default=False,\n",
    "                    help='Use unidirectional LSTM instead of bidirectional')\n",
    "parser.add_argument('--nfft', type=int, default=4096,\n",
    "                    help='STFT fft size and window size')\n",
    "parser.add_argument('--nhop', type=int, default=1024,\n",
    "                    help='STFT hop size')\n",
    "parser.add_argument('--hidden-size', type=int, default=512,\n",
    "                    help='hidden size parameter of dense bottleneck layers')\n",
    "parser.add_argument('--bandwidth', type=int, default=16000,\n",
    "                    help='maximum model bandwidth in herz')\n",
    "parser.add_argument('--nb-channels', type=int, default=2,\n",
    "                    help='set number of channels for model (1, 2)')\n",
    "parser.add_argument('--nb-workers', type=int, default=0,\n",
    "                    help='Number of workers for dataloader.')\n",
    "\n",
    "# Misc Parameters\n",
    "parser.add_argument('--quiet', action='store_true', default=False,\n",
    "                    help='less verbose during training')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='disables CUDA training')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: True\n",
      "Using Torchaudio:  False\n"
     ]
    }
   ],
   "source": [
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "print(\"Using GPU:\", use_cuda)\n",
    "print(\"Using Torchaudio: \", utils._torchaudio_available())\n",
    "dataloader_kwargs = {'num_workers': args.nb_workers, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "repo_dir = os.path.abspath(os.path.dirname('/media/Sharedata/aniruddha/source_sep_Ani/train.py'))\n",
    "repo = Repo(repo_dir)\n",
    "commit = repo.head.commit.hexsha[:7]\n",
    "\n",
    "# use jpg or npy\n",
    "torch.manual_seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--target TARGET]\n",
      "                             [--dataset {musdb,aligned,sourcefolder,trackfolder_var,trackfolder_fix}]\n",
      "                             [--root ROOT] [--output OUTPUT] [--model MODEL]\n",
      "                             [--epochs EPOCHS] [--batch-size BATCH_SIZE]\n",
      "                             [--lr LR] [--patience PATIENCE]\n",
      "                             [--lr-decay-patience LR_DECAY_PATIENCE]\n",
      "                             [--lr-decay-gamma LR_DECAY_GAMMA]\n",
      "                             [--weight-decay WEIGHT_DECAY] [--seed S]\n",
      "                             [--seq-dur SEQ_DUR] [--unidirectional]\n",
      "                             [--nfft NFFT] [--nhop NHOP]\n",
      "                             [--hidden-size HIDDEN_SIZE]\n",
      "                             [--bandwidth BANDWIDTH]\n",
      "                             [--nb-channels NB_CHANNELS]\n",
      "                             [--nb-workers NB_WORKERS] [--quiet] [--no-cuda]\n",
      "                             [--is_wav]\n",
      "                             [--samples-per-track SAMPLES_PER_TRACK]\n",
      "                             [--source-augmentations SOURCE_AUGMENTATIONS [SOURCE_AUGMENTATIONS ...]]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /run/user/1010/jupyter/kernel-6f92f0d2-2b32-47ff-b0d1-fcb0ec0d17f7.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:3304: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "train_dataset, valid_dataset, args = data.load_datasets(parser, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-6b99f4ea0a38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m train_sampler = torch.utils.data.DataLoader(\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;34m**\u001b[0m\u001b[0mdataloader_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "target_path = Path(args.output)\n",
    "target_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_sampler = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=args.batch_size, shuffle=True,\n",
    "    **dataloader_kwargs\n",
    ")\n",
    "valid_sampler = torch.utils.data.DataLoader(\n",
    "    valid_dataset, batch_size=1,\n",
    "    **dataloader_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-7ff06faf56a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.add_argument('--iswav', action='store_true', default=True, help='loads wav instead of STEMS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/99 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "t = tqdm.trange(1,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.set_postfix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import museval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'museval' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2e46eab43dcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmuseval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_mus_track\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'museval' is not defined"
     ]
    }
   ],
   "source": [
    "museval.eval_mus_track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import model\n",
    "import data\n",
    "import torch\n",
    "import time\n",
    "from pathlib import Path\n",
    "import tqdm\n",
    "import json\n",
    "import utils\n",
    "import sklearn.preprocessing\n",
    "import numpy as np\n",
    "import random\n",
    "from git import Repo\n",
    "import os\n",
    "import copy\n",
    "\n",
    "\n",
    "tqdm.monitor_interval = 0\n",
    "\n",
    "\n",
    "def train(args, unmix, device, train_sampler, optimizer):\n",
    "    losses = utils.AverageMeter()\n",
    "    unmix.train()\n",
    "    pbar = tqdm.tqdm(train_sampler, disable=args.quiet) \n",
    "    for x, y in pbar:\n",
    "        pbar.set_description(\"Training batch\")\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        Y_hat = unmix(x)\n",
    "        Y = unmix.transform(y)\n",
    "        loss = torch.nn.functional.mse_loss(Y_hat, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.update(loss.item(), Y.size(1))\n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def valid(args, unmix, device, valid_sampler):\n",
    "    losses = utils.AverageMeter()\n",
    "    unmix.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in valid_sampler:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            Y_hat = unmix(x)\n",
    "            Y = unmix.transform(y)\n",
    "            loss = torch.nn.functional.mse_loss(Y_hat, Y)\n",
    "            losses.update(loss.item(), Y.size(1))\n",
    "        return losses.avg\n",
    "\n",
    "\n",
    "def get_statistics(args, dataset):\n",
    "    scaler = sklearn.preprocessing.StandardScaler()\n",
    "\n",
    "    spec = torch.nn.Sequential(\n",
    "        model.STFT(n_fft=args.nfft, n_hop=args.nhop),\n",
    "        model.Spectrogram(mono=False)\n",
    "    )\n",
    "\n",
    "    dataset_scaler = copy.deepcopy(dataset)\n",
    "    dataset_scaler.samples_per_track = 1\n",
    "    dataset_scaler.augmentations = None\n",
    "    dataset_scaler.random_chunks = False\n",
    "    dataset_scaler.seq_duration = None\n",
    "    pbar = tqdm.tqdm(range(len(dataset_scaler)), disable=args.quiet)\n",
    "    for ind in pbar:\n",
    "        x, y = dataset_scaler[ind]\n",
    "        pbar.set_description(\"Compute dataset statistics\")\n",
    "        X = spec(x[None, ...])\n",
    "        print(\"HELLO\", np.squeeze(X)[:,0])\n",
    "        scaler.partial_fit(np.squeeze(X))\n",
    "\n",
    "    # set inital input scaler values\n",
    "    std = np.maximum(\n",
    "        scaler.scale_,\n",
    "        1e-4*np.max(scaler.scale_)\n",
    "    )\n",
    "    return scaler.mean_, std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: True\n",
      "Using Torchaudio:  False\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Open Unmix Trainer')\n",
    "\n",
    "# which target do we want to train?\n",
    "parser.add_argument('--target', type=str, default='vocals',\n",
    "                    help='target source (will be passed to the dataset)')\n",
    "\n",
    "# Dataset paramaters\n",
    "parser.add_argument('--dataset', type=str, default=\"aligned\",\n",
    "                    choices=[\n",
    "                        'musdb', 'aligned', 'sourcefolder',\n",
    "                        'trackfolder_var', 'trackfolder_fix'\n",
    "                    ],\n",
    "                    help='Name of the dataset.')\n",
    "parser.add_argument('--root', type=str, help='root path of dataset', default='../rec_data/')\n",
    "parser.add_argument('--output', type=str, default=\"../out_unmix/\",\n",
    "                    help='provide output path base folder name')\n",
    "parser.add_argument('--model', type=str, help='Path to checkpoint folder')\n",
    "\n",
    "# Trainig Parameters\n",
    "parser.add_argument('--epochs', type=int, default=1000)\n",
    "parser.add_argument('--batch-size', type=int, default=16)\n",
    "parser.add_argument('--lr', type=float, default=0.01,\n",
    "                    help='learning rate, defaults to 1e-3')\n",
    "parser.add_argument('--patience', type=int, default=140,\n",
    "                    help='maximum number of epochs to train (default: 140)')\n",
    "parser.add_argument('--lr-decay-patience', type=int, default=80,\n",
    "                    help='lr decay patience for plateau scheduler')\n",
    "parser.add_argument('--lr-decay-gamma', type=float, default=0.999,\n",
    "                    help='gamma of learning rate scheduler decay')\n",
    "parser.add_argument('--weight-decay', type=float, default=0.00001,\n",
    "                    help='weight decay')\n",
    "parser.add_argument('--seed', type=int, default=42, metavar='S',\n",
    "                    help='random seed (default: 42)')\n",
    "\n",
    "# Model Parameters\n",
    "parser.add_argument('--seq-dur', type=float, default=6.0,\n",
    "                    help='Sequence duration in seconds'\n",
    "                    'value of <=0.0 will use full/variable length')\n",
    "parser.add_argument('--unidirectional', action='store_true', default=False,\n",
    "                    help='Use unidirectional LSTM instead of bidirectional')\n",
    "parser.add_argument('--nfft', type=int, default=4096,\n",
    "                    help='STFT fft size and window size')\n",
    "parser.add_argument('--nhop', type=int, default=1024,\n",
    "                    help='STFT hop size')\n",
    "parser.add_argument('--hidden-size', type=int, default=512,\n",
    "                    help='hidden size parameter of dense bottleneck layers')\n",
    "parser.add_argument('--bandwidth', type=int, default=16000,\n",
    "                    help='maximum model bandwidth in herz')\n",
    "parser.add_argument('--nb-channels', type=int, default=2,\n",
    "                    help='set number of channels for model (1, 2)')\n",
    "parser.add_argument('--nb-workers', type=int, default=8,\n",
    "                    help='Number of workers for dataloader.')\n",
    "\n",
    "# Misc Parameters\n",
    "parser.add_argument('--quiet', action='store_true', default=False,\n",
    "                    help='less verbose during training')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='disables CUDA training')\n",
    "\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "print(\"Using GPU:\", use_cuda)\n",
    "print(\"Using Torchaudio: \", utils._torchaudio_available())\n",
    "dataloader_kwargs = {'num_workers': args.nb_workers, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "# repo_dir = os.path.abspath(os.path.dirname(__file__))\n",
    "# repo = Repo(repo_dir)\n",
    "# commit = repo.head.commit.hexsha[:7]\n",
    "\n",
    "# use jpg or npy\n",
    "torch.manual_seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArgumentError",
     "evalue": "argument --input-file: conflicting option string: --input-file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArgumentError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-4035d2ed470f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/media/Sharedata/aniruddha/source_sep_Ani/data.py\u001b[0m in \u001b[0;36mload_datasets\u001b[0;34m(parser, args)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \"\"\"\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'aligned'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--input-file'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mixture.wav'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--output-file'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'vocals.wav'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/argparse.py\u001b[0m in \u001b[0;36madd_argument\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1359\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"length of metavar tuple does not match nargs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd_argument_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/argparse.py\u001b[0m in \u001b[0;36m_add_action\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m   1722\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_add_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1723\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption_strings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1724\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optionals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1725\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1726\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_positionals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/argparse.py\u001b[0m in \u001b[0;36m_add_action\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_add_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1565\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ArgumentGroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1566\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_group_actions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/argparse.py\u001b[0m in \u001b[0;36m_add_action\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m   1373\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_add_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m         \u001b[0;31m# resolve any conflicts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_conflict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m         \u001b[0;31m# add to actions list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/argparse.py\u001b[0m in \u001b[0;36m_check_conflict\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m   1512\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfl_optionals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m             \u001b[0mconflict_handler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m             \u001b[0mconflict_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfl_optionals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_handle_conflict_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconflicting_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/argparse.py\u001b[0m in \u001b[0;36m_handle_conflict_error\u001b[0;34m(self, action, conflicting_actions)\u001b[0m\n\u001b[1;32m   1521\u001b[0m                                      \u001b[0;32mfor\u001b[0m \u001b[0moption_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m                                      in conflicting_actions])\n\u001b[0;32m-> 1523\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mArgumentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mconflict_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1525\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_handle_conflict_resolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconflicting_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mArgumentError\u001b[0m: argument --input-file: conflicting option string: --input-file"
     ]
    }
   ],
   "source": [
    "train_dataset, valid_dataset, args = data.load_datasets(parser, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stft_f = torch.randn(2,3,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stft_f = torch.randn(1,2,2049,5000,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 2049, 5000, 2])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stft_f.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.mean(stft_f,1,keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 2049, 5000, 2])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "stft_f = stft_f.transpose(2, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "stft_f = stft_f.pow(2).sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 5000, 2049])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stft_f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stft_f = torch.mean(stft_f, 1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 5000, 2049])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stft_f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = stft_f.permute(2,0,1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 1, 2, 2049])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
