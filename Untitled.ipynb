{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import model\n",
    "import data\n",
    "import torch\n",
    "import time\n",
    "from pathlib import Path\n",
    "import tqdm\n",
    "import json\n",
    "import utils\n",
    "import sklearn.preprocessing\n",
    "import numpy as np\n",
    "import random\n",
    "from git import Repo\n",
    "import os\n",
    "import copy\n",
    "\n",
    "\n",
    "tqdm.monitor_interval = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, unmix, device, train_sampler, optimizer):\n",
    "    losses = utils.AverageMeter()\n",
    "    unmix.train()\n",
    "    pbar = tqdm.tqdm(train_sampler, disable=args.quiet) \n",
    "    for x, y in pbar:\n",
    "        pbar.set_description(\"Training batch\")\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        Y_hat = unmix(x)\n",
    "        Y = unmix.transform(y)\n",
    "        loss = torch.nn.functional.mse_loss(Y_hat, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.update(loss.item(), Y.size(1))\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(args, unmix, device, valid_sampler):\n",
    "    losses = utils.AverageMeter()\n",
    "    unmix.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in valid_sampler:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            Y_hat = unmix(x)\n",
    "            Y = unmix.transform(y)\n",
    "            loss = torch.nn.functional.mse_loss(Y_hat, Y)\n",
    "            losses.update(loss.item(), Y.size(1))\n",
    "        return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statistics(args, dataset):\n",
    "    scaler = sklearn.preprocessing.StandardScaler()\n",
    "\n",
    "    spec = torch.nn.Sequential(\n",
    "        model.STFT(n_fft=args.nfft, n_hop=args.nhop),\n",
    "        model.Spectrogram(mono=True)\n",
    "    )\n",
    "\n",
    "    dataset_scaler = copy.deepcopy(dataset)\n",
    "    dataset_scaler.samples_per_track = 1\n",
    "    dataset_scaler.augmentations = None\n",
    "    dataset_scaler.random_chunks = False\n",
    "    dataset_scaler.seq_duration = None\n",
    "    pbar = tqdm.tqdm(range(len(dataset_scaler)), disable=args.quiet)\n",
    "    for ind in pbar:\n",
    "        x, y = dataset_scaler[ind]\n",
    "        pbar.set_description(\"Compute dataset statistics\")\n",
    "        X = spec(x[None, ...])\n",
    "        scaler.partial_fit(np.squeeze(X))\n",
    "\n",
    "    # set inital input scaler values\n",
    "    std = np.maximum(\n",
    "        scaler.scale_,\n",
    "        1e-4*np.max(scaler.scale_)\n",
    "    )\n",
    "    return scaler.mean_, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreTrueAction(option_strings=['--no-cuda'], dest='no_cuda', nargs=0, const=True, default=False, type=None, choices=None, help='disables CUDA training', metavar=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Open Unmix Trainer')\n",
    "\n",
    "# which target do we want to train?\n",
    "parser.add_argument('--target', type=str, default='vocals',\n",
    "                    help='target source (will be passed to the dataset)')\n",
    "\n",
    "# Dataset paramaters\n",
    "parser.add_argument('--dataset', type=str, default=\"musdb\",\n",
    "                    choices=[\n",
    "                        'musdb', 'aligned', 'sourcefolder',\n",
    "                        'trackfolder_var', 'trackfolder_fix'\n",
    "                    ],\n",
    "                    help='Name of the dataset.')\n",
    "parser.add_argument('--root', type=str, help='root path of dataset' , default='../rec_data/')\n",
    "parser.add_argument('--output', type=str, default=\"../out_unmix/\",\n",
    "                    help='provide output path base folder name')\n",
    "parser.add_argument('--model', type=str, help='Path to checkpoint folder', default='../checkpoint_unmix')\n",
    "\n",
    "# Trainig Parameters\n",
    "parser.add_argument('--epochs', type=int, default=1000)\n",
    "parser.add_argument('--batch-size', type=int, default=16)\n",
    "parser.add_argument('--lr', type=float, default=0.001,\n",
    "                    help='learning rate, defaults to 1e-3')\n",
    "parser.add_argument('--patience', type=int, default=140,\n",
    "                    help='maximum number of epochs to train (default: 140)')\n",
    "parser.add_argument('--lr-decay-patience', type=int, default=80,\n",
    "                    help='lr decay patience for plateau scheduler')\n",
    "parser.add_argument('--lr-decay-gamma', type=float, default=0.3,\n",
    "                    help='gamma of learning rate scheduler decay')\n",
    "parser.add_argument('--weight-decay', type=float, default=0.00001,\n",
    "                    help='weight decay')\n",
    "parser.add_argument('--seed', type=int, default=42, metavar='S',\n",
    "                    help='random seed (default: 42)')\n",
    "\n",
    "\n",
    "# Model Parameters\n",
    "parser.add_argument('--seq-dur', type=float, default=6.0,\n",
    "                    help='Sequence duration in seconds'\n",
    "                    'value of <=0.0 will use full/variable length')\n",
    "parser.add_argument('--unidirectional', action='store_true', default=False,\n",
    "                    help='Use unidirectional LSTM instead of bidirectional')\n",
    "parser.add_argument('--nfft', type=int, default=4096,\n",
    "                    help='STFT fft size and window size')\n",
    "parser.add_argument('--nhop', type=int, default=1024,\n",
    "                    help='STFT hop size')\n",
    "parser.add_argument('--hidden-size', type=int, default=512,\n",
    "                    help='hidden size parameter of dense bottleneck layers')\n",
    "parser.add_argument('--bandwidth', type=int, default=16000,\n",
    "                    help='maximum model bandwidth in herz')\n",
    "parser.add_argument('--nb-channels', type=int, default=2,\n",
    "                    help='set number of channels for model (1, 2)')\n",
    "parser.add_argument('--nb-workers', type=int, default=0,\n",
    "                    help='Number of workers for dataloader.')\n",
    "\n",
    "# Misc Parameters\n",
    "parser.add_argument('--quiet', action='store_true', default=False,\n",
    "                    help='less verbose during training')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='disables CUDA training')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: True\n",
      "Using Torchaudio:  False\n"
     ]
    }
   ],
   "source": [
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "print(\"Using GPU:\", use_cuda)\n",
    "print(\"Using Torchaudio: \", utils._torchaudio_available())\n",
    "dataloader_kwargs = {'num_workers': args.nb_workers, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "repo_dir = os.path.abspath(os.path.dirname('/media/Sharedata/aniruddha/source_sep_Ani/train.py'))\n",
    "repo = Repo(repo_dir)\n",
    "commit = repo.head.commit.hexsha[:7]\n",
    "\n",
    "# use jpg or npy\n",
    "torch.manual_seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--target TARGET]\n",
      "                             [--dataset {musdb,aligned,sourcefolder,trackfolder_var,trackfolder_fix}]\n",
      "                             [--root ROOT] [--output OUTPUT] [--model MODEL]\n",
      "                             [--epochs EPOCHS] [--batch-size BATCH_SIZE]\n",
      "                             [--lr LR] [--patience PATIENCE]\n",
      "                             [--lr-decay-patience LR_DECAY_PATIENCE]\n",
      "                             [--lr-decay-gamma LR_DECAY_GAMMA]\n",
      "                             [--weight-decay WEIGHT_DECAY] [--seed S]\n",
      "                             [--seq-dur SEQ_DUR] [--unidirectional]\n",
      "                             [--nfft NFFT] [--nhop NHOP]\n",
      "                             [--hidden-size HIDDEN_SIZE]\n",
      "                             [--bandwidth BANDWIDTH]\n",
      "                             [--nb-channels NB_CHANNELS]\n",
      "                             [--nb-workers NB_WORKERS] [--quiet] [--no-cuda]\n",
      "                             [--is_wav]\n",
      "                             [--samples-per-track SAMPLES_PER_TRACK]\n",
      "                             [--source-augmentations SOURCE_AUGMENTATIONS [SOURCE_AUGMENTATIONS ...]]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /run/user/1010/jupyter/kernel-6f92f0d2-2b32-47ff-b0d1-fcb0ec0d17f7.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:3304: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "train_dataset, valid_dataset, args = data.load_datasets(parser, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-6b99f4ea0a38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m train_sampler = torch.utils.data.DataLoader(\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;34m**\u001b[0m\u001b[0mdataloader_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "target_path = Path(args.output)\n",
    "target_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_sampler = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=args.batch_size, shuffle=True,\n",
    "    **dataloader_kwargs\n",
    ")\n",
    "valid_sampler = torch.utils.data.DataLoader(\n",
    "    valid_dataset, batch_size=1,\n",
    "    **dataloader_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-7ff06faf56a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.add_argument('--iswav', action='store_true', default=True, help='loads wav instead of STEMS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/99 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "t = tqdm.trange(1,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.set_postfix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import museval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'museval' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2e46eab43dcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmuseval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_mus_track\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'museval' is not defined"
     ]
    }
   ],
   "source": [
    "museval.eval_mus_track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import model\n",
    "import data\n",
    "import torch\n",
    "import time\n",
    "from pathlib import Path\n",
    "import tqdm\n",
    "import json\n",
    "import utils\n",
    "import sklearn.preprocessing\n",
    "import numpy as np\n",
    "import random\n",
    "from git import Repo\n",
    "import os\n",
    "import copy\n",
    "\n",
    "\n",
    "tqdm.monitor_interval = 0\n",
    "\n",
    "\n",
    "def train(args, unmix, device, train_sampler, optimizer):\n",
    "    losses = utils.AverageMeter()\n",
    "    unmix.train()\n",
    "    pbar = tqdm.tqdm(train_sampler, disable=args.quiet) \n",
    "    for x, y in pbar:\n",
    "        pbar.set_description(\"Training batch\")\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        Y_hat = unmix(x)\n",
    "        Y = unmix.transform(y)\n",
    "        loss = torch.nn.functional.mse_loss(Y_hat, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.update(loss.item(), Y.size(1))\n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def valid(args, unmix, device, valid_sampler):\n",
    "    losses = utils.AverageMeter()\n",
    "    unmix.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in valid_sampler:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            Y_hat = unmix(x)\n",
    "            Y = unmix.transform(y)\n",
    "            loss = torch.nn.functional.mse_loss(Y_hat, Y)\n",
    "            losses.update(loss.item(), Y.size(1))\n",
    "        return losses.avg\n",
    "\n",
    "\n",
    "def get_statistics(args, dataset):\n",
    "    scaler = sklearn.preprocessing.StandardScaler()\n",
    "\n",
    "    spec = torch.nn.Sequential(\n",
    "        model.STFT(n_fft=args.nfft, n_hop=args.nhop),\n",
    "        model.Spectrogram(mono=False)\n",
    "    )\n",
    "\n",
    "    dataset_scaler = copy.deepcopy(dataset)\n",
    "    dataset_scaler.samples_per_track = 1\n",
    "    dataset_scaler.augmentations = None\n",
    "    dataset_scaler.random_chunks = False\n",
    "    dataset_scaler.seq_duration = None\n",
    "    pbar = tqdm.tqdm(range(len(dataset_scaler)), disable=args.quiet)\n",
    "    for ind in pbar:\n",
    "        x, y = dataset_scaler[ind]\n",
    "        pbar.set_description(\"Compute dataset statistics\")\n",
    "        X = spec(x[None, ...])\n",
    "        print(\"HELLO\", np.squeeze(X)[:,0])\n",
    "        scaler.partial_fit(np.squeeze(X))\n",
    "\n",
    "    # set inital input scaler values\n",
    "    std = np.maximum(\n",
    "        scaler.scale_,\n",
    "        1e-4*np.max(scaler.scale_)\n",
    "    )\n",
    "    return scaler.mean_, std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: True\n",
      "Using Torchaudio:  False\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Open Unmix Trainer')\n",
    "\n",
    "# which target do we want to train?\n",
    "parser.add_argument('--target', type=str, default='vocals',\n",
    "                    help='target source (will be passed to the dataset)')\n",
    "\n",
    "# Dataset paramaters\n",
    "parser.add_argument('--dataset', type=str, default=\"aligned\",\n",
    "                    choices=[\n",
    "                        'musdb', 'aligned', 'sourcefolder',\n",
    "                        'trackfolder_var', 'trackfolder_fix'\n",
    "                    ],\n",
    "                    help='Name of the dataset.')\n",
    "parser.add_argument('--root', type=str, help='root path of dataset', default='../rec_data/')\n",
    "parser.add_argument('--output', type=str, default=\"../out_unmix/\",\n",
    "                    help='provide output path base folder name')\n",
    "parser.add_argument('--model', type=str, help='Path to checkpoint folder')\n",
    "\n",
    "# Trainig Parameters\n",
    "parser.add_argument('--epochs', type=int, default=1000)\n",
    "parser.add_argument('--batch-size', type=int, default=16)\n",
    "parser.add_argument('--lr', type=float, default=0.01,\n",
    "                    help='learning rate, defaults to 1e-3')\n",
    "parser.add_argument('--patience', type=int, default=140,\n",
    "                    help='maximum number of epochs to train (default: 140)')\n",
    "parser.add_argument('--lr-decay-patience', type=int, default=80,\n",
    "                    help='lr decay patience for plateau scheduler')\n",
    "parser.add_argument('--lr-decay-gamma', type=float, default=0.999,\n",
    "                    help='gamma of learning rate scheduler decay')\n",
    "parser.add_argument('--weight-decay', type=float, default=0.00001,\n",
    "                    help='weight decay')\n",
    "parser.add_argument('--seed', type=int, default=42, metavar='S',\n",
    "                    help='random seed (default: 42)')\n",
    "\n",
    "# Model Parameters\n",
    "parser.add_argument('--seq-dur', type=float, default=6.0,\n",
    "                    help='Sequence duration in seconds'\n",
    "                    'value of <=0.0 will use full/variable length')\n",
    "parser.add_argument('--unidirectional', action='store_true', default=False,\n",
    "                    help='Use unidirectional LSTM instead of bidirectional')\n",
    "parser.add_argument('--nfft', type=int, default=4096,\n",
    "                    help='STFT fft size and window size')\n",
    "parser.add_argument('--nhop', type=int, default=1024,\n",
    "                    help='STFT hop size')\n",
    "parser.add_argument('--hidden-size', type=int, default=512,\n",
    "                    help='hidden size parameter of dense bottleneck layers')\n",
    "parser.add_argument('--bandwidth', type=int, default=16000,\n",
    "                    help='maximum model bandwidth in herz')\n",
    "parser.add_argument('--nb-channels', type=int, default=2,\n",
    "                    help='set number of channels for model (1, 2)')\n",
    "parser.add_argument('--nb-workers', type=int, default=8,\n",
    "                    help='Number of workers for dataloader.')\n",
    "\n",
    "# Misc Parameters\n",
    "parser.add_argument('--quiet', action='store_true', default=False,\n",
    "                    help='less verbose during training')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='disables CUDA training')\n",
    "\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "print(\"Using GPU:\", use_cuda)\n",
    "print(\"Using Torchaudio: \", utils._torchaudio_available())\n",
    "dataloader_kwargs = {'num_workers': args.nb_workers, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "# repo_dir = os.path.abspath(os.path.dirname(__file__))\n",
    "# repo = Repo(repo_dir)\n",
    "# commit = repo.head.commit.hexsha[:7]\n",
    "\n",
    "# use jpg or npy\n",
    "torch.manual_seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArgumentError",
     "evalue": "argument --input-file: conflicting option string: --input-file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArgumentError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-4035d2ed470f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/media/Sharedata/aniruddha/source_sep_Ani/data.py\u001b[0m in \u001b[0;36mload_datasets\u001b[0;34m(parser, args)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \"\"\"\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'aligned'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--input-file'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mixture.wav'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--output-file'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'vocals.wav'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/argparse.py\u001b[0m in \u001b[0;36madd_argument\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1359\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"length of metavar tuple does not match nargs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd_argument_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/argparse.py\u001b[0m in \u001b[0;36m_add_action\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m   1722\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_add_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1723\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption_strings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1724\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optionals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1725\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1726\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_positionals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/argparse.py\u001b[0m in \u001b[0;36m_add_action\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_add_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1565\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ArgumentGroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1566\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_group_actions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/argparse.py\u001b[0m in \u001b[0;36m_add_action\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m   1373\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_add_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m         \u001b[0;31m# resolve any conflicts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_conflict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m         \u001b[0;31m# add to actions list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/argparse.py\u001b[0m in \u001b[0;36m_check_conflict\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m   1512\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfl_optionals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m             \u001b[0mconflict_handler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m             \u001b[0mconflict_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfl_optionals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_handle_conflict_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconflicting_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/argparse.py\u001b[0m in \u001b[0;36m_handle_conflict_error\u001b[0;34m(self, action, conflicting_actions)\u001b[0m\n\u001b[1;32m   1521\u001b[0m                                      \u001b[0;32mfor\u001b[0m \u001b[0moption_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m                                      in conflicting_actions])\n\u001b[0;32m-> 1523\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mArgumentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mconflict_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1525\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_handle_conflict_resolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconflicting_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mArgumentError\u001b[0m: argument --input-file: conflicting option string: --input-file"
     ]
    }
   ],
   "source": [
    "train_dataset, valid_dataset, args = data.load_datasets(parser, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stft_f = torch.randn(2,3,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stft_f = torch.randn(1,2,2049,5000,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 2049, 5000, 2])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stft_f.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.mean(stft_f,1,keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 2049, 5000, 2])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "stft_f = stft_f.transpose(2, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "stft_f = stft_f.pow(2).sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 5000, 2049])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stft_f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stft_f = torch.mean(stft_f, 1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 5000, 2049])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stft_f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = stft_f.permute(2,0,1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 1, 2, 2049])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randn((100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = scipy.signal.find_peaks(a, height=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5,  7, 26, 32, 50, 53, 56, 60, 69, 71, 74, 77, 82, 87, 90, 92, 94,\n",
       "       96, 98])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.53071896, -1.1036617 , -0.40160848, -1.14950537, -0.37462194,\n",
       "        1.5922571 ,  0.58171191,  0.85255842, -0.08164452,  0.11810969,\n",
       "       -1.36122751, -1.75818078, -0.93611475, -0.07888777, -0.45290947,\n",
       "       -0.19270839,  0.31505188,  0.58493627,  0.28739222,  0.60990167,\n",
       "       -1.24741701,  0.39521685, -2.20564813, -0.63203404,  0.58645671,\n",
       "        0.41597205,  1.850311  ,  0.06861904,  0.44558096, -1.52554165,\n",
       "       -0.01792022, -0.02869859,  1.72709509,  1.37722648, -0.92499077,\n",
       "        0.39369934, -0.19574454, -0.29916275, -0.66679583, -1.28462171,\n",
       "        0.63253362, -0.40714917, -0.66344988,  0.17892144,  0.68713829,\n",
       "       -0.81843673,  0.04856925, -0.18903787, -1.05907236, -0.06054354,\n",
       "        0.7490058 , -1.64417144,  0.81804329,  1.05734784,  0.38137929,\n",
       "       -1.53598169,  1.08593923, -0.35388386, -0.50031001, -0.30774408,\n",
       "        1.48581909,  0.44517201, -1.45478934, -2.36063491, -0.47860069,\n",
       "        0.68089923, -0.602861  , -1.00018192, -0.45593662,  0.94713616,\n",
       "       -0.96606147,  1.53618131, -1.07366227,  0.10218223,  0.7265953 ,\n",
       "        0.5074559 ,  0.66767693,  1.16899304,  0.40597446, -0.51998293,\n",
       "       -0.0725128 ,  0.45116245,  0.90219345, -0.23723414, -0.93094917,\n",
       "       -1.77564919,  0.9200853 ,  1.00306918, -1.25211837, -0.34147023,\n",
       "        2.19595439,  0.02130418,  2.21818841, -2.16673861,  0.99846364,\n",
       "       -0.21228518,  0.87076649, -1.02938937,  1.13009969, -2.50237999])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(a.shape[0]):\n",
    "    if x in b[0]:\n",
    "        a[x] = 1\n",
    "    else:\n",
    "        a[x] = 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0.])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bin(x, thresh):\n",
    "    y = x.numpy()\n",
    "    #print(type(y))\n",
    "    z = scipy.signal.find_peaks(y, height=thresh)\n",
    "    for i in range(x.shape[0]):\n",
    "        if i in z[0]:\n",
    "            y[i] = 1\n",
    "        else:\n",
    "            y[i] = 0\n",
    "    return torch.from_numpy(y)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randn((111))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.from_numpy(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
       "        1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
       "        0., 1., 0.], dtype=torch.float64)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_bin(b, thresh=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n"
     ]
    }
   ],
   "source": [
    "for i in range(111):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
