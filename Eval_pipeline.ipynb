{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "import argparse\n",
    "import musdb\n",
    "import museval\n",
    "import test\n",
    "import multiprocessing\n",
    "import functools\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import json\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import crepe\n",
    "import scipy.io.wavfile\n",
    "import mir_eval\n",
    "import essentia.standard as ess\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Within each experiment folder, there will be folders corresponding to the models being evaluated\n",
    "2. Within the folder for each model there will be a folder corresponding to each test example\n",
    "3. Within folder for each test example we will save the model output (vocals and accompaniments) as well as json file containing the evaluation metrics for that example and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create folder within given experiment corresponding to model being evaluated\n",
    "exp_no = 7\n",
    "model = '../new_models/test/model_tabla_mse_pretrain1/' #Path to the saved model\n",
    "model_name = 'model_tabla_mse_pretrain1'\n",
    "\n",
    "exp_model_res_path = '../test_out/Exp_' + str(exp_no) + '/' + model_name + '/'\n",
    "\n",
    "if not(os.path.exists(exp_model_res_path)):\n",
    "    os.mkdir(exp_model_res_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create folders corresponding to each test examples \n",
    "test_data_folder = '../rec_data_final/test/'\n",
    "for test_eg in os.listdir(test_data_folder):\n",
    "    test_eg_path = exp_model_res_path + test_eg + '/'\n",
    "    if not(os.path.exists(test_eg_path)):\n",
    "        os.mkdir(test_eg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find output of the model for each test example and save in corresponding folder\n",
    "# for test_eg in os.listdir(test_data_folder):\n",
    "#     #print(test_eg)\n",
    "#     test_eg_source = test_data_folder + test_eg #path to test data\n",
    "#     test_eg_eval = exp_model_res_path + test_eg + '/'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Obtain SDR values for each test example as well as save estimate audios\n",
    "# Define required functions\n",
    "def pad_or_truncate(\n",
    "    audio_reference,\n",
    "    audio_estimates\n",
    "):\n",
    "    \"\"\"Pad or truncate estimates by duration of references:\n",
    "    - If reference > estimates: add zeros at the and of the estimated signal\n",
    "    - If estimates > references: truncate estimates to duration of references\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    references : np.ndarray, shape=(nsrc, nsampl, nchan)\n",
    "        array containing true reference sources\n",
    "    estimates : np.ndarray, shape=(nsrc, nsampl, nchan)\n",
    "        array containing estimated sources\n",
    "    Returns\n",
    "    -------\n",
    "    references : np.ndarray, shape=(nsrc, nsampl, nchan)\n",
    "        array containing true reference sources\n",
    "    estimates : np.ndarray, shape=(nsrc, nsampl, nchan)\n",
    "        array containing estimated sources\n",
    "    \"\"\"\n",
    "    est_shape = audio_estimates.shape\n",
    "    ref_shape = audio_reference.shape\n",
    "    if est_shape[1] != ref_shape[1]:\n",
    "        if est_shape[1] >= ref_shape[1]:\n",
    "            audio_estimates = audio_estimates[:, :ref_shape[1], :]\n",
    "        else:\n",
    "            # pad end with zeros\n",
    "            audio_estimates = np.pad(\n",
    "                audio_estimates,\n",
    "                [\n",
    "                    (0, 0),\n",
    "                    (0, ref_shape[1] - est_shape[1]),\n",
    "                    (0, 0)\n",
    "                ],\n",
    "                mode='constant'\n",
    "            )\n",
    "\n",
    "    return audio_reference, audio_estimates\n",
    "\n",
    "def evaluate(\n",
    "    references,\n",
    "    estimates,\n",
    "    win=1*44100,\n",
    "    hop=1*44100,\n",
    "    mode='v4',\n",
    "    padding=True\n",
    "):\n",
    "    \"\"\"BSS_EVAL images evaluation using metrics module\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    references : np.ndarray, shape=(nsrc, nsampl, nchan)\n",
    "        array containing true reference sources\n",
    "    estimates : np.ndarray, shape=(nsrc, nsampl, nchan)\n",
    "        array containing estimated sources\n",
    "    window : int, defaults to 44100\n",
    "        window size in samples\n",
    "    hop : int\n",
    "        hop size in samples, defaults to 44100 (no overlap)\n",
    "    mode : str\n",
    "        BSSEval version, default to `v4`\n",
    "    Returns\n",
    "    -------\n",
    "    SDR : np.ndarray, shape=(nsrc,)\n",
    "        vector of Signal to Distortion Ratios (SDR)\n",
    "    ISR : np.ndarray, shape=(nsrc,)\n",
    "        vector of Source to Spatial Distortion Image (ISR)\n",
    "    SIR : np.ndarray, shape=(nsrc,)\n",
    "        vector of Source to Interference Ratios (SIR)\n",
    "    SAR : np.ndarray, shape=(nsrc,)\n",
    "        vector of Sources to Artifacts Ratios (SAR)\n",
    "    \"\"\"\n",
    "\n",
    "    estimates = np.array(estimates)\n",
    "    references = np.array(references)\n",
    "\n",
    "    if padding:\n",
    "        references, estimates = pad_or_truncate(references, estimates)\n",
    "\n",
    "    SDR, ISR, SIR, SAR, _ = museval.metrics.bss_eval(\n",
    "        references,\n",
    "        estimates,\n",
    "        compute_permutation=False,\n",
    "        window=win,\n",
    "        hop=hop,\n",
    "        framewise_filters=(mode == \"v3\"),\n",
    "        bsseval_sources_version=False\n",
    "    )\n",
    "\n",
    "    return SDR, ISR, SIR, SAR\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#targets = ['vocals']\n",
    "targets = ['tabla']\n",
    "root = '../rec_data_final/'\n",
    "subset = 'test'\n",
    "cores = 1\n",
    "no_cuda = False\n",
    "is_wav = True\n",
    "samplerate = 44100\n",
    "use_cuda = not no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mus = musdb.DB(\n",
    "    root=root,\n",
    "    download=root is None,\n",
    "    subsets=subset,\n",
    "    is_wav=is_wav\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Darbari_10dB\n",
      "CHECK OpenUnmix(\n",
      "  (stft): STFT()\n",
      "  (spec): Spectrogram()\n",
      "  (transform): Sequential(\n",
      "    (0): STFT()\n",
      "    (1): Spectrogram()\n",
      "  )\n",
      "  (fc1): Linear(in_features=2974, out_features=512, bias=False)\n",
      "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (lstm): LSTM(512, 256, num_layers=3, dropout=0.4, bidirectional=True)\n",
      "  (fc2): Linear(in_features=1024, out_features=512, bias=False)\n",
      "  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=512, out_features=4098, bias=False)\n",
      "  (bn3): BatchNorm1d(4098, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.97s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED SEPARATED VOCALS AND ACCOMPANIMENTS!\n",
      "Darbari_15dB\n",
      "CHECK OpenUnmix(\n",
      "  (stft): STFT()\n",
      "  (spec): Spectrogram()\n",
      "  (transform): Sequential(\n",
      "    (0): STFT()\n",
      "    (1): Spectrogram()\n",
      "  )\n",
      "  (fc1): Linear(in_features=2974, out_features=512, bias=False)\n",
      "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (lstm): LSTM(512, 256, num_layers=3, dropout=0.4, bidirectional=True)\n",
      "  (fc2): Linear(in_features=1024, out_features=512, bias=False)\n",
      "  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=512, out_features=4098, bias=False)\n",
      "  (bn3): BatchNorm1d(4098, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.13s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED SEPARATED VOCALS AND ACCOMPANIMENTS!\n",
      "Darbari_20dB\n",
      "CHECK OpenUnmix(\n",
      "  (stft): STFT()\n",
      "  (spec): Spectrogram()\n",
      "  (transform): Sequential(\n",
      "    (0): STFT()\n",
      "    (1): Spectrogram()\n",
      "  )\n",
      "  (fc1): Linear(in_features=2974, out_features=512, bias=False)\n",
      "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (lstm): LSTM(512, 256, num_layers=3, dropout=0.4, bidirectional=True)\n",
      "  (fc2): Linear(in_features=1024, out_features=512, bias=False)\n",
      "  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=512, out_features=4098, bias=False)\n",
      "  (bn3): BatchNorm1d(4098, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.18s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED SEPARATED VOCALS AND ACCOMPANIMENTS!\n",
      "Deepak-pooriya dhanashri_10dB\n",
      "CHECK OpenUnmix(\n",
      "  (stft): STFT()\n",
      "  (spec): Spectrogram()\n",
      "  (transform): Sequential(\n",
      "    (0): STFT()\n",
      "    (1): Spectrogram()\n",
      "  )\n",
      "  (fc1): Linear(in_features=2974, out_features=512, bias=False)\n",
      "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (lstm): LSTM(512, 256, num_layers=3, dropout=0.4, bidirectional=True)\n",
      "  (fc2): Linear(in_features=1024, out_features=512, bias=False)\n",
      "  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=512, out_features=4098, bias=False)\n",
      "  (bn3): BatchNorm1d(4098, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.21s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED SEPARATED VOCALS AND ACCOMPANIMENTS!\n",
      "Deepak-pooriya dhanashri_15dB\n",
      "CHECK OpenUnmix(\n",
      "  (stft): STFT()\n",
      "  (spec): Spectrogram()\n",
      "  (transform): Sequential(\n",
      "    (0): STFT()\n",
      "    (1): Spectrogram()\n",
      "  )\n",
      "  (fc1): Linear(in_features=2974, out_features=512, bias=False)\n",
      "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (lstm): LSTM(512, 256, num_layers=3, dropout=0.4, bidirectional=True)\n",
      "  (fc2): Linear(in_features=1024, out_features=512, bias=False)\n",
      "  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=512, out_features=4098, bias=False)\n",
      "  (bn3): BatchNorm1d(4098, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.73s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED SEPARATED VOCALS AND ACCOMPANIMENTS!\n",
      "Deepak-pooriya dhanashri_20dB\n",
      "CHECK OpenUnmix(\n",
      "  (stft): STFT()\n",
      "  (spec): Spectrogram()\n",
      "  (transform): Sequential(\n",
      "    (0): STFT()\n",
      "    (1): Spectrogram()\n",
      "  )\n",
      "  (fc1): Linear(in_features=2974, out_features=512, bias=False)\n",
      "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (lstm): LSTM(512, 256, num_layers=3, dropout=0.4, bidirectional=True)\n",
      "  (fc2): Linear(in_features=1024, out_features=512, bias=False)\n",
      "  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=512, out_features=4098, bias=False)\n",
      "  (bn3): BatchNorm1d(4098, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.56s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED SEPARATED VOCALS AND ACCOMPANIMENTS!\n",
      "Deepak_jog_kauns_10dB\n",
      "CHECK OpenUnmix(\n",
      "  (stft): STFT()\n",
      "  (spec): Spectrogram()\n",
      "  (transform): Sequential(\n",
      "    (0): STFT()\n",
      "    (1): Spectrogram()\n",
      "  )\n",
      "  (fc1): Linear(in_features=2974, out_features=512, bias=False)\n",
      "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (lstm): LSTM(512, 256, num_layers=3, dropout=0.4, bidirectional=True)\n",
      "  (fc2): Linear(in_features=1024, out_features=512, bias=False)\n",
      "  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=512, out_features=4098, bias=False)\n",
      "  (bn3): BatchNorm1d(4098, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.63s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED SEPARATED VOCALS AND ACCOMPANIMENTS!\n",
      "Deepak_jog_kauns_15dB\n",
      "CHECK OpenUnmix(\n",
      "  (stft): STFT()\n",
      "  (spec): Spectrogram()\n",
      "  (transform): Sequential(\n",
      "    (0): STFT()\n",
      "    (1): Spectrogram()\n",
      "  )\n",
      "  (fc1): Linear(in_features=2974, out_features=512, bias=False)\n",
      "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (lstm): LSTM(512, 256, num_layers=3, dropout=0.4, bidirectional=True)\n",
      "  (fc2): Linear(in_features=1024, out_features=512, bias=False)\n",
      "  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=512, out_features=4098, bias=False)\n",
      "  (bn3): BatchNorm1d(4098, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.92s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED SEPARATED VOCALS AND ACCOMPANIMENTS!\n",
      "Deepak_jog_kauns_20dB\n",
      "CHECK OpenUnmix(\n",
      "  (stft): STFT()\n",
      "  (spec): Spectrogram()\n",
      "  (transform): Sequential(\n",
      "    (0): STFT()\n",
      "    (1): Spectrogram()\n",
      "  )\n",
      "  (fc1): Linear(in_features=2974, out_features=512, bias=False)\n",
      "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (lstm): LSTM(512, 256, num_layers=3, dropout=0.4, bidirectional=True)\n",
      "  (fc2): Linear(in_features=1024, out_features=512, bias=False)\n",
      "  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=512, out_features=4098, bias=False)\n",
      "  (bn3): BatchNorm1d(4098, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.57s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED SEPARATED VOCALS AND ACCOMPANIMENTS!\n",
      "Deepak_maari_bihag_10dB\n",
      "CHECK OpenUnmix(\n",
      "  (stft): STFT()\n",
      "  (spec): Spectrogram()\n",
      "  (transform): Sequential(\n",
      "    (0): STFT()\n",
      "    (1): Spectrogram()\n",
      "  )\n",
      "  (fc1): Linear(in_features=2974, out_features=512, bias=False)\n",
      "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (lstm): LSTM(512, 256, num_layers=3, dropout=0.4, bidirectional=True)\n",
      "  (fc2): Linear(in_features=1024, out_features=512, bias=False)\n",
      "  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=512, out_features=4098, bias=False)\n",
      "  (bn3): BatchNorm1d(4098, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.89s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED SEPARATED VOCALS AND ACCOMPANIMENTS!\n",
      "Deepak_maari_bihag_15dB\n",
      "CHECK OpenUnmix(\n",
      "  (stft): STFT()\n",
      "  (spec): Spectrogram()\n",
      "  (transform): Sequential(\n",
      "    (0): STFT()\n",
      "    (1): Spectrogram()\n",
      "  )\n",
      "  (fc1): Linear(in_features=2974, out_features=512, bias=False)\n",
      "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (lstm): LSTM(512, 256, num_layers=3, dropout=0.4, bidirectional=True)\n",
      "  (fc2): Linear(in_features=1024, out_features=512, bias=False)\n",
      "  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=512, out_features=4098, bias=False)\n",
      "  (bn3): BatchNorm1d(4098, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.82s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED SEPARATED VOCALS AND ACCOMPANIMENTS!\n",
      "Deepak_maari_bihag_20dB\n",
      "CHECK OpenUnmix(\n",
      "  (stft): STFT()\n",
      "  (spec): Spectrogram()\n",
      "  (transform): Sequential(\n",
      "    (0): STFT()\n",
      "    (1): Spectrogram()\n",
      "  )\n",
      "  (fc1): Linear(in_features=2974, out_features=512, bias=False)\n",
      "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (lstm): LSTM(512, 256, num_layers=3, dropout=0.4, bidirectional=True)\n",
      "  (fc2): Linear(in_features=1024, out_features=512, bias=False)\n",
      "  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=512, out_features=4098, bias=False)\n",
      "  (bn3): BatchNorm1d(4098, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.75s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED SEPARATED VOCALS AND ACCOMPANIMENTS!\n",
      "YAMAN_ALAP_10dB\n",
      "CHECK OpenUnmix(\n",
      "  (stft): STFT()\n",
      "  (spec): Spectrogram()\n",
      "  (transform): Sequential(\n",
      "    (0): STFT()\n",
      "    (1): Spectrogram()\n",
      "  )\n",
      "  (fc1): Linear(in_features=2974, out_features=512, bias=False)\n",
      "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (lstm): LSTM(512, 256, num_layers=3, dropout=0.4, bidirectional=True)\n",
      "  (fc2): Linear(in_features=1024, out_features=512, bias=False)\n",
      "  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=512, out_features=4098, bias=False)\n",
      "  (bn3): BatchNorm1d(4098, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.77s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED SEPARATED VOCALS AND ACCOMPANIMENTS!\n",
      "YAMAN_ALAP_15dB\n",
      "CHECK OpenUnmix(\n",
      "  (stft): STFT()\n",
      "  (spec): Spectrogram()\n",
      "  (transform): Sequential(\n",
      "    (0): STFT()\n",
      "    (1): Spectrogram()\n",
      "  )\n",
      "  (fc1): Linear(in_features=2974, out_features=512, bias=False)\n",
      "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (lstm): LSTM(512, 256, num_layers=3, dropout=0.4, bidirectional=True)\n",
      "  (fc2): Linear(in_features=1024, out_features=512, bias=False)\n",
      "  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=512, out_features=4098, bias=False)\n",
      "  (bn3): BatchNorm1d(4098, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.66s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED SEPARATED VOCALS AND ACCOMPANIMENTS!\n",
      "YAMAN_ALAP_20dB\n",
      "CHECK OpenUnmix(\n",
      "  (stft): STFT()\n",
      "  (spec): Spectrogram()\n",
      "  (transform): Sequential(\n",
      "    (0): STFT()\n",
      "    (1): Spectrogram()\n",
      "  )\n",
      "  (fc1): Linear(in_features=2974, out_features=512, bias=False)\n",
      "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (lstm): LSTM(512, 256, num_layers=3, dropout=0.4, bidirectional=True)\n",
      "  (fc2): Linear(in_features=1024, out_features=512, bias=False)\n",
      "  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=512, out_features=4098, bias=False)\n",
      "  (bn3): BatchNorm1d(4098, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.67s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED SEPARATED VOCALS AND ACCOMPANIMENTS!\n",
      "Yaman_res_10dB\n",
      "CHECK OpenUnmix(\n",
      "  (stft): STFT()\n",
      "  (spec): Spectrogram()\n",
      "  (transform): Sequential(\n",
      "    (0): STFT()\n",
      "    (1): Spectrogram()\n",
      "  )\n",
      "  (fc1): Linear(in_features=2974, out_features=512, bias=False)\n",
      "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (lstm): LSTM(512, 256, num_layers=3, dropout=0.4, bidirectional=True)\n",
      "  (fc2): Linear(in_features=1024, out_features=512, bias=False)\n",
      "  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=512, out_features=4098, bias=False)\n",
      "  (bn3): BatchNorm1d(4098, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:06<00:00,  6.21s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED SEPARATED VOCALS AND ACCOMPANIMENTS!\n",
      "Yaman_res_15dB\n",
      "CHECK OpenUnmix(\n",
      "  (stft): STFT()\n",
      "  (spec): Spectrogram()\n",
      "  (transform): Sequential(\n",
      "    (0): STFT()\n",
      "    (1): Spectrogram()\n",
      "  )\n",
      "  (fc1): Linear(in_features=2974, out_features=512, bias=False)\n",
      "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (lstm): LSTM(512, 256, num_layers=3, dropout=0.4, bidirectional=True)\n",
      "  (fc2): Linear(in_features=1024, out_features=512, bias=False)\n",
      "  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=512, out_features=4098, bias=False)\n",
      "  (bn3): BatchNorm1d(4098, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:06<00:00,  6.02s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED SEPARATED VOCALS AND ACCOMPANIMENTS!\n",
      "Yaman_res_20dB\n",
      "CHECK OpenUnmix(\n",
      "  (stft): STFT()\n",
      "  (spec): Spectrogram()\n",
      "  (transform): Sequential(\n",
      "    (0): STFT()\n",
      "    (1): Spectrogram()\n",
      "  )\n",
      "  (fc1): Linear(in_features=2974, out_features=512, bias=False)\n",
      "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (lstm): LSTM(512, 256, num_layers=3, dropout=0.4, bidirectional=True)\n",
      "  (fc2): Linear(in_features=1024, out_features=512, bias=False)\n",
      "  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=512, out_features=4098, bias=False)\n",
      "  (bn3): BatchNorm1d(4098, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:06<00:00,  6.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED SEPARATED VOCALS AND ACCOMPANIMENTS!\n"
     ]
    }
   ],
   "source": [
    "# iterate over all tracks present in test folder\n",
    "for track in mus.tracks:\n",
    "    outdir = exp_model_res_path + track.name + '/'    \n",
    "    print(track.name)\n",
    "    estimates = test.separate(\n",
    "        audio=track.audio,\n",
    "        targets=targets,\n",
    "        model_name=model,\n",
    "        niter=2,\n",
    "        alpha=1,\n",
    "        softmask=False,\n",
    "        #device=device\n",
    "    )    \n",
    "    \n",
    "    for target, estimate in estimates.items():\n",
    "        sf.write(\n",
    "            outdir / Path(target).with_suffix('.wav'),\n",
    "            estimate,\n",
    "            samplerate\n",
    "        )\n",
    "    \n",
    "    print(\"SAVED SEPARATED VOCALS AND ACCOMPANIMENTS!\")\n",
    "    \n",
    "#     audio_estimates = []\n",
    "#     audio_reference = []\n",
    "#     eval_targets = []\n",
    "\n",
    "#     for key, target in list(track.targets.items()):\n",
    "#         try:\n",
    "#             # try to fetch the audio from the user_results of a given key\n",
    "#             estimates[key]\n",
    "#         except KeyError:\n",
    "#             # ignore wrong key and continue\n",
    "#             continue\n",
    "#         eval_targets.append(key)\n",
    "        \n",
    "#     mode='v4'\n",
    "#     win=1.0\n",
    "#     hop=1.0\n",
    "#     data = museval.aggregate.TrackStore(win=win, hop=hop, track_name=track.name)\n",
    "\n",
    "#     # check if vocals and accompaniment is among the targets\n",
    "#     #has_acc = all(x in eval_targets for x in ['vocals', 'accompaniment'])\n",
    "#     has_acc = all(x in eval_targets for x in ['tabla', 'accompaniment'])\n",
    "#     if has_acc:\n",
    "#         # remove accompaniment from list of targets, because\n",
    "#         # the voc/acc scenario will be evaluated separately\n",
    "#         eval_targets.remove('accompaniment')\n",
    "        \n",
    "#     #audio_estimates.append(estimates['vocals'])\n",
    "#     #audio_reference.append(track.targets['vocals'].audio)\n",
    "    \n",
    "#     audio_estimates.append(estimates['tabla'])\n",
    "#     audio_reference.append(track.targets['tabla'].audio)\n",
    "    \n",
    "    \n",
    "#     SDR, ISR, SIR, SAR = evaluate(\n",
    "#             audio_reference,\n",
    "#             audio_estimates,\n",
    "#             win=int(win*track.rate),\n",
    "#             hop=int(hop*track.rate),\n",
    "#             mode=mode\n",
    "#         )\n",
    "    \n",
    "#     save_dict = {}\n",
    "#     save_dict['SDR'] = SDR[0].tolist()\n",
    "#     save_dict['ISR'] = ISR[0].tolist()\n",
    "#     save_dict['SDR_median'] = np.median(SDR[0])\n",
    "#     save_dict['ISR_median'] = np.median(ISR[0])\n",
    "    \n",
    "#     save_file = outdir + \"evaluation.json\"\n",
    "    \n",
    "#     print(\"Saving json file\")\n",
    "    \n",
    "#     with open(save_file, 'w') as outfile:\n",
    "#         json.dump(save_dict, outfile)\n",
    "\n",
    "#     print(\"Saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin evaluation of pitch accuracy (PYIN)\n",
    "W = 1024\n",
    "H = 512\n",
    "\n",
    "for track in os.listdir(exp_model_res_path):\n",
    "    print(track)\n",
    "    print(\"Extracting Pitch\")\n",
    "    \n",
    "    clean_vocs_path = test_data_folder + track + '/vocals.wav'\n",
    "    extr_vocs_path = exp_model_res_path + track + '/vocals.wav'\n",
    "    \n",
    "    clean_vocs = ess.MonoLoader(sampleRate=44100 , filename=clean_vocs_path)()\n",
    "    extr_vocs = ess.MonoLoader(sampleRate=44100 , filename=extr_vocs_path)()\n",
    "\n",
    "    #     t_clean_vocs, f_clean_vocs, conf_clean_vocs, act_clean_vocs = crepe.predict(clean_vocs, r1, viterbi=True)\n",
    "    #     t_extr_vocs, f_extr_vocs, conf_extr_vocs, act_extr_vocs = crepe.predict(extr_vocs, r2, viterbi=True)\n",
    "\n",
    "    pyin = ess.PitchYinProbabilistic(frameSize=W, hopSize=H)\n",
    "\n",
    "    f_clean_vocs, conf_clean_vocs = pyin(clean_vocs)\n",
    "    f_extr_vocs, conf_extr_vocs = pyin(extr_vocs)\n",
    "    \n",
    "    print(\"Pitch extracted!\")\n",
    "    \n",
    "    \n",
    "    #Make entire frequency array postivie\n",
    "    f_clean_vocs = np.absolute(f_clean_vocs)\n",
    "    f_extr_vocs = np.absolute(f_extr_vocs)    \n",
    "\n",
    "    if (f_clean_vocs.shape[0]>f_extr_vocs.shape[0]):\n",
    "        f_clean_vocs = f_clean_vocs[:f_extr_vocs.shape[0]]\n",
    "        conf_clean_vocs = conf_clean_vocs[:f_extr_vocs.shape[0]]\n",
    "        #t_clean_vocs = t_extr_vocs\n",
    "    elif (f_clean_vocs.shape[0]<=f_extr_vocs.shape[0]):\n",
    "        f_extr_vocs = f_extr_vocs[:f_clean_vocs.shape[0]]\n",
    "        conf_extr_vocs = conf_extr_vocs[:f_clean_vocs.shape[0]]\n",
    "        #t_extr_vocs = t_clean_vocs\n",
    "    \n",
    "    #plt.plot((f_clean_vocs-f_extr_vocs)**2)\n",
    "    #plt.plot(f_clean_vocs)\n",
    "    #plt.show()\n",
    "    \n",
    "    #Convert frequency to cents\n",
    "    f_clean_vocs_cents = mir_eval.melody.hz2cents(f_clean_vocs)\n",
    "    f_extr_vocs_cents = mir_eval.melody.hz2cents(f_extr_vocs)\n",
    "    \n",
    "    plt.plot((f_clean_vocs_cents-f_extr_vocs_cents)**2)\n",
    "    outdir = exp_model_res_path + track + '/'\n",
    "    plt.savefig(outdir + \"freq_mse.png\")\n",
    "    \n",
    "#     np.savetxt(outdir+'clean_pitch.csv', f_clean_vocs, delimiter=',')\n",
    "#     np.savetxt(outdir+'extr_pitch.csv', f_extr_vocs , delimiter=',')\n",
    "\n",
    "\n",
    "    #Convert voicing probabilites to binary vector, with a given threshold\n",
    "    conf_clean_vocs_bin = np.zeros(np.shape(conf_clean_vocs))\n",
    "    conf_extr_vocs_bin = np.zeros(np.shape(conf_extr_vocs))\n",
    "    thresh = 0.5\n",
    "    for i in range(conf_clean_vocs_bin.shape[0]):\n",
    "        if (conf_clean_vocs[i]>thresh):\n",
    "            conf_clean_vocs_bin[i]=1\n",
    "        if (conf_extr_vocs[i]>thresh):\n",
    "            conf_extr_vocs_bin[i]=1\n",
    "            \n",
    "    print(\"Calculating pitch accuracy measures\")\n",
    "    \n",
    "    #Voicing accuracy measures    \n",
    "    v_rec, fal_al = mir_eval.melody.voicing_measures(conf_clean_vocs_bin, conf_extr_vocs_bin)\n",
    "\n",
    "    #Calculate raw pitch accuracy - Default cent_tolerance=50\n",
    "    rpa = mir_eval.melody.raw_pitch_accuracy(conf_clean_vocs_bin, f_clean_vocs_cents, conf_extr_vocs_bin, f_extr_vocs_cents)\n",
    "\n",
    "    #Calculate raw chroma accuracy - Default cent_tolerance=50\n",
    "    rca = mir_eval.melody.raw_chroma_accuracy(conf_clean_vocs_bin, f_clean_vocs_cents, conf_extr_vocs_bin, f_extr_vocs_cents)\n",
    "\n",
    "    #Calculate overall accuracy\n",
    "    ov_acc = mir_eval.melody.overall_accuracy(conf_clean_vocs_bin, f_clean_vocs_cents, conf_extr_vocs_bin, f_extr_vocs_cents)\n",
    "\n",
    "    outdir = exp_model_res_path + track + '/'\n",
    "    save_file = outdir + \"pitch_evaluation.json\"\n",
    "    \n",
    "    #Save pitch evaluations in json file\n",
    "    pitch_track_eval = {}\n",
    "    pitch_track_eval['Voicing recall'] = v_rec\n",
    "    pitch_track_eval['False alarm'] = fal_al\n",
    "    pitch_track_eval['Raw pitch accuracy'] = rpa\n",
    "    pitch_track_eval['Raw chroma accuracy'] = rca\n",
    "    pitch_track_eval['Overall accuracy'] = ov_acc\n",
    "\n",
    "#     with open(save_file, 'w') as fp:\n",
    "#         json.dump(pitch_track_eval, fp)\n",
    "    \n",
    "#     print(\"Pitch accuracy measures saved!\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin evaluation of pitch accuracy (CREPE)\n",
    "W = 1024\n",
    "H = 512\n",
    "\n",
    "for track in os.listdir(exp_model_res_path):\n",
    "    print(track)\n",
    "    print(\"Extracting Pitch\")\n",
    "    \n",
    "    clean_vocs_path = test_data_folder + track + '/vocals.wav'\n",
    "    extr_vocs_path = exp_model_res_path + track + '/vocals.wav'\n",
    "    \n",
    "    clean_vocs = ess.MonoLoader(sampleRate=44100 , filename=clean_vocs_path)()\n",
    "    extr_vocs = ess.MonoLoader(sampleRate=44100 , filename=extr_vocs_path)()\n",
    "\n",
    "    t_clean_vocs, f_clean_vocs, conf_clean_vocs, act_clean_vocs = crepe.predict(clean_vocs, 44100, viterbi=True)\n",
    "    t_extr_vocs, f_extr_vocs, conf_extr_vocs, act_extr_vocs = crepe.predict(extr_vocs, 44100, viterbi=True)\n",
    "\n",
    "#     pyin = ess.PitchYinProbabilistic(frameSize=W, hopSize=H)\n",
    "\n",
    "#     f_clean_vocs, conf_clean_vocs = pyin(clean_vocs)\n",
    "#     f_extr_vocs, conf_extr_vocs = pyin(extr_vocs)\n",
    "\n",
    "    print(f_clean_vocs.shape)\n",
    "    print(t_clean_vocs.shape)\n",
    "    \n",
    "    print(\"Pitch extracted!\")\n",
    "    \n",
    "    \n",
    "    #Make entire frequency array postivie\n",
    "    f_clean_vocs = np.absolute(f_clean_vocs)\n",
    "    f_extr_vocs = np.absolute(f_extr_vocs)    \n",
    "\n",
    "    if (f_clean_vocs.shape[0]>f_extr_vocs.shape[0]):\n",
    "        f_clean_vocs = f_clean_vocs[:f_extr_vocs.shape[0]]\n",
    "        conf_clean_vocs = conf_clean_vocs[:f_extr_vocs.shape[0]]\n",
    "        t_clean_vocs = t_extr_vocs\n",
    "    elif (f_clean_vocs.shape[0]<=f_extr_vocs.shape[0]):\n",
    "        f_extr_vocs = f_extr_vocs[:f_clean_vocs.shape[0]]\n",
    "        conf_extr_vocs = conf_extr_vocs[:f_clean_vocs.shape[0]]\n",
    "        t_extr_vocs = t_clean_vocs\n",
    "    \n",
    "    #plt.plot((f_clean_vocs-f_extr_vocs)**2)\n",
    "    #plt.plot(f_clean_vocs)\n",
    "    #plt.show()\n",
    "    \n",
    "    #Convert frequency to cents\n",
    "    f_clean_vocs_cents = mir_eval.melody.hz2cents(f_clean_vocs)\n",
    "    f_extr_vocs_cents = mir_eval.melody.hz2cents(f_extr_vocs)\n",
    "    \n",
    "    plt.plot((f_clean_vocs_cents-f_extr_vocs_cents)**2)\n",
    "    outdir = exp_model_res_path + track + '/'\n",
    "    plt.savefig(outdir + \"freq_mse_CREPE.png\")\n",
    "\n",
    "    clean_pitch_track =  np.transpose(np.concatenate((np.atleast_2d(t_clean_vocs), np.atleast_2d(f_clean_vocs)), axis=0))\n",
    "    extr_pitch_track =  np.transpose(np.concatenate((np.atleast_2d(t_extr_vocs), np.atleast_2d(f_extr_vocs)), axis=0))\n",
    "\n",
    "    np.savetxt(outdir+'clean_pitch_crepe.csv', clean_pitch_track, delimiter=',')\n",
    "    np.savetxt(outdir+'extr_pitch_crepe.csv', extr_pitch_track , delimiter=',')\n",
    "    \n",
    "    \n",
    "    \n",
    "#     np.savetxt(outdir+'clean_pitch.csv', f_clean_vocs, delimiter=',')\n",
    "#     np.savetxt(outdir+'extr_pitch.csv', f_extr_vocs , delimiter=',')\n",
    "\n",
    "\n",
    "    #Convert voicing probabilites to binary vector, with a given threshold\n",
    "    conf_clean_vocs_bin = np.zeros(np.shape(conf_clean_vocs))\n",
    "    conf_extr_vocs_bin = np.zeros(np.shape(conf_extr_vocs))\n",
    "    thresh = 0.5\n",
    "    for i in range(conf_clean_vocs_bin.shape[0]):\n",
    "        if (conf_clean_vocs[i]>thresh):\n",
    "            conf_clean_vocs_bin[i]=1\n",
    "        if (conf_extr_vocs[i]>thresh):\n",
    "            conf_extr_vocs_bin[i]=1\n",
    "            \n",
    "    print(\"Calculating pitch accuracy measures\")\n",
    "    \n",
    "    #Voicing accuracy measures    \n",
    "    v_rec, fal_al = mir_eval.melody.voicing_measures(conf_clean_vocs_bin, conf_extr_vocs_bin)\n",
    "\n",
    "    #Calculate raw pitch accuracy - Default cent_tolerance=50\n",
    "    rpa = mir_eval.melody.raw_pitch_accuracy(conf_clean_vocs_bin, f_clean_vocs_cents, conf_extr_vocs_bin, f_extr_vocs_cents)\n",
    "\n",
    "    #Calculate raw chroma accuracy - Default cent_tolerance=50\n",
    "    rca = mir_eval.melody.raw_chroma_accuracy(conf_clean_vocs_bin, f_clean_vocs_cents, conf_extr_vocs_bin, f_extr_vocs_cents)\n",
    "\n",
    "    #Calculate overall accuracy\n",
    "    ov_acc = mir_eval.melody.overall_accuracy(conf_clean_vocs_bin, f_clean_vocs_cents, conf_extr_vocs_bin, f_extr_vocs_cents)\n",
    "\n",
    "    outdir = exp_model_res_path + track + '/'\n",
    "    save_file = outdir + \"pitch_evaluation_CREPE.json\"\n",
    "    \n",
    "    #Save pitch evaluations in json file\n",
    "    pitch_track_eval = {}\n",
    "    pitch_track_eval['Voicing recall'] = v_rec\n",
    "    pitch_track_eval['False alarm'] = fal_al\n",
    "    pitch_track_eval['Raw pitch accuracy'] = rpa\n",
    "    pitch_track_eval['Raw chroma accuracy'] = rca\n",
    "    pitch_track_eval['Overall accuracy'] = ov_acc\n",
    "\n",
    "#     with open(save_file, 'w') as fp:\n",
    "#         json.dump(pitch_track_eval, fp)\n",
    "    \n",
    "#     print(\"Pitch accuracy measures saved!\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_clean_vocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track = m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
